---
title: "Practical Machine Learning Course Project"
author: "Joseph Imperato"
date: "1/29/2020"
output: html_document
---
Introduction:  The authors of the “Qualitative Activity Recognition of Weight Lifting Exercises” article demonstrate a methodology to leverage recordings from sensors on individuals performing certain exercises to determine the quality of the execution of the exercise.  My project will use this recorded information to train a model to predict, based on a set of recordings, how well an exercise was executed.

Data Preparation:  The first step will be to read the csv file into R for pre- processing.  The training data will be stored in traindat and the test data will be testdat.  Then the data will be analyzed using R commands such as dim(), head() [output not shown] and table().  These commands are listed below. 

```{r}
library(caret);
library(randomForest);
traindat = read.csv("/Users/imperj/Desktop/pml-training.csv")
testdat = read.csv("/Users/imperj/Desktop/pml-testing.csv")
mydata <- as.data.frame(traindat)
mytestdata <- as.data.frame(testdat)
dim(mydata)
dim(mytestdata)
table(mydata$classe)
```
Data Pre-processing:  The head() command showed that there were many columns with mostly no information or NA.  The commands below will remove the columns with either blank or NA in the fields.

```{r}
mydf <- Filter(function(x)!all(is.na(x) || x == ""), mydata)
mytestdf <- Filter(function(x)!all(is.na(x) || x == ""), mytestdata)
```
The analysis is also based on the available sensor information so variables that don’t have sensor information like X, Name, and timestamp and window data are not required for the analysis and therefore are removed by the commands below.
```{r}
mydf <- mydf[,-c(1:7)]
mytestdf <- mytestdf[,-c(1:7)]
dim(mydf)
dim(mytestdf)
```
Data Cross Validation:  The next step will be to prepare the data for training.  One phase of this is cross validation where I further split up the training set into training and test data sets so we can develop and test different models and then select the best performing model (see commands below).  We would then use this model on the true test data.  I did not break up the training set further as the models I used, implement their own re-sampling techniques (see below)

```{r}
inTrain <- createDataPartition(y=mydf$classe,p=.75,list=F)
training <- mydf[inTrain,]
testing <- mydf[-inTrain,]
```
Machine Learning Models:  The models I used, as referenced in the literature, were bagging and random forest.  Bagging is a technique were the predication model is performed many times on re-sampled data and the best model is selected.  I used the “treebag” method which uses decision trees to develop the prediction model.   Shown below it did boostraps/resampling 25 times before developing the final model.  
```{r}
fitrf <- train(classe~.,method= "treebag", data= training)
fitrf
```
Bagging Prediction:  The prediction effectiveness using the bagging model “fitrf” is shown below via the confusionMatrix command.   The accuracy >98% predicting across all 5 variables.  The error rates were expected to be very low based on the training run accuracy.
```{r}
prediction1 <-predict(fitrf,testing)
confusionMatrix(prediction1,testing$classe)
```
Random Forest Model.  The next modeling technique I used was random forest.  This model will also use several random sub samples of the data and create a new tree for each sub sample.  It will take the results of the classifications and average together to get the best prediction model.  In this case it fitted ~500 trees to develop a highly accurate model, as shown in the output below.  Note: I used the randomForest() command as the train command was far too slow running on my older windows machine.
```{r}
model1 <- randomForest(classe ~. , data=training, method="class")
model1
```
Random Forest Prediction:  The prediction effectiveness using the random forest model “model1” is shown below via the confusionMatrix command.   The accuracy >99% predicting across all 5 variables.  Similarly the test error rates were expected to be low based on the training run accuracy.
```{r}
prediction2 <- predict(model1, testing, type = "class")
confusionMatrix(prediction2,testing$classe)
```
Predicting on 20 variable test set: I used the random forest model on the test set as it had a higher accuracy score than the bagging model.  The outcome is listed below. The error in the predictions here is how we determine out of sample error rate.  The out of sample error rate is expected to be low given the success of the previous runs on test data

```{r}
prediction3 <- predict(model1, testdat, type = "class")
prediction3
```
The results were then used to answer the quiz questions.
 